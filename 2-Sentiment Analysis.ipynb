{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644bf37b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Hi! Mario here! In this project I'll perform Sentiment Analysis on a Kaggle dataset (https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset/data), which contains the following:\n",
    "\n",
    "1) Text ID\n",
    "2) Text\n",
    "3) Selected text\n",
    "4) Sentiment\n",
    "5) Time of tweet\n",
    "6) Age of User\n",
    "7) Country\n",
    "8) Population\n",
    "9) Land Area\n",
    "10) Density\n",
    "\n",
    "It comes in 2 separate files, one for training and one for testing. Let's first import some packages and load both files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd971a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b9a18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>31072940</td>\n",
       "      <td>227540.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Greece</td>\n",
       "      <td>10423054</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Grenada</td>\n",
       "      <td>112523</td>\n",
       "      <td>340.0</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>17915568</td>\n",
       "      <td>107160.0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>13132795</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \\\n",
       "0                    I`d have responded, if I were going   neutral   \n",
       "1                                               Sooo SAD  negative   \n",
       "2                                            bullying me  negative   \n",
       "3                                         leave me alone  negative   \n",
       "4                                          Sons of ****,  negative   \n",
       "...                                                  ...       ...   \n",
       "27476                                             d lost  negative   \n",
       "27477                                      , don`t force  negative   \n",
       "27478                          Yay good for both of you.  positive   \n",
       "27479                         But it was worth it  ****.  positive   \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral   \n",
       "\n",
       "      Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0           morning        0-20  Afghanistan          38928346   \n",
       "1              noon       21-30      Albania           2877797   \n",
       "2             night       31-45      Algeria          43851044   \n",
       "3           morning       46-60      Andorra             77265   \n",
       "4              noon       60-70       Angola          32866272   \n",
       "...             ...         ...          ...               ...   \n",
       "27476         night       31-45        Ghana          31072940   \n",
       "27477       morning       46-60       Greece          10423054   \n",
       "27478          noon       60-70      Grenada            112523   \n",
       "27479         night      70-100    Guatemala          17915568   \n",
       "27480       morning        0-20       Guinea          13132795   \n",
       "\n",
       "       Land Area (Km²)  Density (P/Km²)  \n",
       "0             652860.0               60  \n",
       "1              27400.0              105  \n",
       "2            2381740.0               18  \n",
       "3                470.0              164  \n",
       "4            1246700.0               26  \n",
       "...                ...              ...  \n",
       "27476         227540.0              137  \n",
       "27477         128900.0               81  \n",
       "27478            340.0              331  \n",
       "27479         107160.0              167  \n",
       "27480         246000.0               53  \n",
       "\n",
       "[27481 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Sentiment_train.csv', encoding='unicode_escape')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b2e44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346.0</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797.0</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044.0</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272.0</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4811</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4812</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4814</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4815 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment  \\\n",
       "0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1     96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3     01082688c6                                        happy bday!  positive   \n",
       "4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "...          ...                                                ...       ...   \n",
       "4810         NaN                                                NaN       NaN   \n",
       "4811         NaN                                                NaN       NaN   \n",
       "4812         NaN                                                NaN       NaN   \n",
       "4813         NaN                                                NaN       NaN   \n",
       "4814         NaN                                                NaN       NaN   \n",
       "\n",
       "     Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0          morning        0-20  Afghanistan        38928346.0   \n",
       "1             noon       21-30      Albania         2877797.0   \n",
       "2            night       31-45      Algeria        43851044.0   \n",
       "3          morning       46-60      Andorra           77265.0   \n",
       "4             noon       60-70       Angola        32866272.0   \n",
       "...            ...         ...          ...               ...   \n",
       "4810           NaN         NaN          NaN               NaN   \n",
       "4811           NaN         NaN          NaN               NaN   \n",
       "4812           NaN         NaN          NaN               NaN   \n",
       "4813           NaN         NaN          NaN               NaN   \n",
       "4814           NaN         NaN          NaN               NaN   \n",
       "\n",
       "      Land Area (Km²)  Density (P/Km²)  \n",
       "0            652860.0             60.0  \n",
       "1             27400.0            105.0  \n",
       "2           2381740.0             18.0  \n",
       "3               470.0            164.0  \n",
       "4           1246700.0             26.0  \n",
       "...               ...              ...  \n",
       "4810              NaN              NaN  \n",
       "4811              NaN              NaN  \n",
       "4812              NaN              NaN  \n",
       "4813              NaN              NaN  \n",
       "4814              NaN              NaN  \n",
       "\n",
       "[4815 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('Sentiment_test.csv', encoding='unicode_escape')\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fbf11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            27481 non-null  object \n",
      " 1   text              27480 non-null  object \n",
      " 2   selected_text     27480 non-null  object \n",
      " 3   sentiment         27481 non-null  object \n",
      " 4   Time of Tweet     27481 non-null  object \n",
      " 5   Age of User       27481 non-null  object \n",
      " 6   Country           27481 non-null  object \n",
      " 7   Population -2020  27481 non-null  int64  \n",
      " 8   Land Area (Km²)   27481 non-null  float64\n",
      " 9   Density (P/Km²)   27481 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd42e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4815 entries, 0 to 4814\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            3534 non-null   object \n",
      " 1   text              3534 non-null   object \n",
      " 2   sentiment         3534 non-null   object \n",
      " 3   Time of Tweet     3534 non-null   object \n",
      " 4   Age of User       3534 non-null   object \n",
      " 5   Country           3534 non-null   object \n",
      " 6   Population -2020  3534 non-null   float64\n",
      " 7   Land Area (Km²)   3534 non-null   float64\n",
      " 8   Density (P/Km²)   3534 non-null   float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 338.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53cb96",
   "metadata": {},
   "source": [
    "Let's see the frequency of each sentiment in the training set. Also, we can see that the test set has some null values in that column, so let's see the null values in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caccae17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f96f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the training set: \n",
      " textID              0\n",
      "text                1\n",
      "selected_text       1\n",
      "sentiment           0\n",
      "Time of Tweet       0\n",
      "Age of User         0\n",
      "Country             0\n",
      "Population -2020    0\n",
      "Land Area (Km²)     0\n",
      "Density (P/Km²)     0\n",
      "dtype: int64\n",
      "\n",
      "Null values in the test set: \n",
      " textID              1281\n",
      "text                1281\n",
      "sentiment           1281\n",
      "Time of Tweet       1281\n",
      "Age of User         1281\n",
      "Country             1281\n",
      "Population -2020    1281\n",
      "Land Area (Km²)     1281\n",
      "Density (P/Km²)     1281\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Null values in the training set: \\n', train.isnull().sum())\n",
    "print('')\n",
    "print('Null values in the test set: \\n', test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd09c7",
   "metadata": {},
   "source": [
    "The classes in the training set are quite balanced. Also, there are a lot of null values in the test set, and only 1 for the train set. Clearly they correspond to important columns in order to perform the sentiment analysis, so we'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcdaa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the training set: \n",
      " textID              0\n",
      "text                0\n",
      "selected_text       0\n",
      "sentiment           0\n",
      "Time of Tweet       0\n",
      "Age of User         0\n",
      "Country             0\n",
      "Population -2020    0\n",
      "Land Area (Km²)     0\n",
      "Density (P/Km²)     0\n",
      "dtype: int64\n",
      "\n",
      "Null values in the test set: \n",
      " textID              0\n",
      "text                0\n",
      "sentiment           0\n",
      "Time of Tweet       0\n",
      "Age of User         0\n",
      "Country             0\n",
      "Population -2020    0\n",
      "Land Area (Km²)     0\n",
      "Density (P/Km²)     0\n",
      "dtype: int64\n",
      "\n",
      "Rows number in the training set:  27480\n",
      "Rows number in the test set:  3534\n"
     ]
    }
   ],
   "source": [
    "train = train.dropna()\n",
    "\n",
    "test = test.dropna()\n",
    "\n",
    "print('Null values in the training set: \\n', train.isnull().sum())\n",
    "print('')\n",
    "print('Null values in the test set: \\n', test.isnull().sum())\n",
    "print('')\n",
    "print('Rows number in the training set: ', len(train))\n",
    "print('Rows number in the test set: ', len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b722d",
   "metadata": {},
   "source": [
    "Now we don't have any null values, we have a training dataset consisting of 27,480 rows and a test dataset of 3,534. Now we can begin the text analysis. First, we need the sentiment labels to be numeric, so I'll create a function that assign the following values: negative = -1, neutral = 0, positive = 1. Then, I'll apply it to both Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259e0c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1   -1\n",
      "2   -1\n",
      "3   -1\n",
      "4   -1\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "0    0\n",
      "1    1\n",
      "2   -1\n",
      "3    1\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def sentiment_encoding(sentiment):\n",
    "    \n",
    "    if sentiment == 'negative':\n",
    "        return -1\n",
    "    \n",
    "    elif sentiment == 'neutral':\n",
    "        return 0\n",
    "    \n",
    "    elif sentiment == 'positive':\n",
    "        return 1\n",
    "    \n",
    "#Apply function to the train and test sets.\n",
    "train['sentiment'] = train['sentiment'].apply(lambda x: sentiment_encoding(x))\n",
    "test['sentiment'] = test['sentiment'].apply(lambda x: sentiment_encoding(x))\n",
    "\n",
    "print(train['sentiment'].head())\n",
    "print('')\n",
    "print(test['sentiment'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c78a7",
   "metadata": {},
   "source": [
    "We now have the sentiment labels numerically encoded. Our next step is to preprocess the text. In the training set I'll just use the raw text, because it's closer to reality. The first thing I'll do is to convert everything to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b49411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: ' '.join(x.lower() for x in str(x).split()))\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join(x.lower() for x in str(x).split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f1d7a",
   "metadata": {},
   "source": [
    "Let's remove the HTML tags and websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36700168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0deb05",
   "metadata": {},
   "source": [
    "Let's remove non alpha-numeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e87f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)])) \n",
    "test['text'] = test['text'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243a667",
   "metadata": {},
   "source": [
    "Now let's remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac10b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "train['text'] = train['text'].apply(lambda x: ' '.join([x for x in x.split() if x not in stop]))\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join([x for x in x.split() if x not in stop]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ba001",
   "metadata": {},
   "source": [
    "Time to lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb9f302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>responded going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>-1</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>bos bullying</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>-1</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>interview leave alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>-1</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>son put release already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>-1</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish could come see u denver husband lost job ...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>-1</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>31072940</td>\n",
       "      <td>227540.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>wondered rake client made clear net force devs...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>-1</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Greece</td>\n",
       "      <td>10423054</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>1</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Grenada</td>\n",
       "      <td>112523</td>\n",
       "      <td>340.0</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>worth</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>17915568</td>\n",
       "      <td>107160.0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>flirting going atg smile yay hug</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>13132795</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                                    responded going   \n",
       "1      549e992a42                            sooo sad miss san diego   \n",
       "2      088c60f138                                       bos bullying   \n",
       "3      9642c003ef                              interview leave alone   \n",
       "4      358bd9e861                     son put release already bought   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0  wish could come see u denver husband lost job ...   \n",
       "27477  4f4c4fc327  wondered rake client made clear net force devs...   \n",
       "27478  f67aae2310  yay good enjoy break probably need hectic week...   \n",
       "27479  ed167662a5                                              worth   \n",
       "27480  6f7127d9d7                   flirting going atg smile yay hug   \n",
       "\n",
       "                                           selected_text  sentiment  \\\n",
       "0                    I`d have responded, if I were going          0   \n",
       "1                                               Sooo SAD         -1   \n",
       "2                                            bullying me         -1   \n",
       "3                                         leave me alone         -1   \n",
       "4                                          Sons of ****,         -1   \n",
       "...                                                  ...        ...   \n",
       "27476                                             d lost         -1   \n",
       "27477                                      , don`t force         -1   \n",
       "27478                          Yay good for both of you.          1   \n",
       "27479                         But it was worth it  ****.          1   \n",
       "27480  All this flirting going on - The ATG smiles. Y...          0   \n",
       "\n",
       "      Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0           morning        0-20  Afghanistan          38928346   \n",
       "1              noon       21-30      Albania           2877797   \n",
       "2             night       31-45      Algeria          43851044   \n",
       "3           morning       46-60      Andorra             77265   \n",
       "4              noon       60-70       Angola          32866272   \n",
       "...             ...         ...          ...               ...   \n",
       "27476         night       31-45        Ghana          31072940   \n",
       "27477       morning       46-60       Greece          10423054   \n",
       "27478          noon       60-70      Grenada            112523   \n",
       "27479         night      70-100    Guatemala          17915568   \n",
       "27480       morning        0-20       Guinea          13132795   \n",
       "\n",
       "       Land Area (Km²)  Density (P/Km²)  \n",
       "0             652860.0               60  \n",
       "1              27400.0              105  \n",
       "2            2381740.0               18  \n",
       "3                470.0              164  \n",
       "4            1246700.0               26  \n",
       "...                ...              ...  \n",
       "27476         227540.0              137  \n",
       "27477         128900.0               81  \n",
       "27478            340.0              331  \n",
       "27479         107160.0              167  \n",
       "27480         246000.0               53  \n",
       "\n",
       "[27480 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train['text'] = train['text'].apply(lambda x: \n",
    "                                            ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "test['text'] = test['text'].apply(lambda x: \n",
    "                                            ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73053a21",
   "metadata": {},
   "source": [
    "We can't classify the text unless it has a numeric format. In order to achieve that, I'll use the method called TF-IDF, which is a numeric measure that expresses how important is a word for a document in a collection. Its use is more recommended than the normal Count Vectorizer. First we convert the training and test texts to a list format and then apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "951083a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['text'].astype(str).tolist()\n",
    "test_text = test['text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecc71777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 2200, binary = True)\n",
    "\n",
    "#Create the X_train and X_test sets.\n",
    "X_train = vectorizer.fit_transform(train_text).toarray()\n",
    "\n",
    "X_test = vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15b04de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 2200)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf285c38",
   "metadata": {},
   "source": [
    "Let's create the target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00d97155",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d153d50",
   "metadata": {},
   "source": [
    "Now I'll try 3 models, a Logistic Regression, a Multinomial Naive Bayes (widely used in text mining) and a Random Forest, for different values of C, alpha and maximum depth - min samples leaf, respectively. Both Logistic Regression and Random Forest will have a balanced class weight. From there I'll decide which one performs better. First, a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46c27732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print('Confusion matrix train set: \\n', confusion_matrix(y_train, y_pred_train))\n",
    "    print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred))\n",
    "    print(' ')\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict = True)\n",
    "    report_test = classification_report(y_test, y_pred, output_dict = True)\n",
    "    \n",
    "    labels = [-1, 0, 1]\n",
    "    precision_train = [report_train['-1']['precision'], report_train['0']['precision'], report_train['1']['precision']]\n",
    "    precision_test = [report_test['-1']['precision'], report_test['0']['precision'], report_test['1']['precision']]\n",
    "    \n",
    "    recall_train = [report_train['-1']['recall'], report_train['0']['recall'], report_train['1']['recall']]\n",
    "    recall_test = [report_test['-1']['recall'], report_test['0']['recall'], report_test['1']['recall']]\n",
    "    \n",
    "    F1score_train = [report_train['-1']['f1-score'], report_train['0']['f1-score'], report_train['1']['f1-score']]\n",
    "    F1score_test = [report_test['-1']['f1-score'], report_test['0']['f1-score'], report_test['1']['f1-score']]\n",
    "    \n",
    "    accuracy_train = [report_train['accuracy'], report_train['accuracy'], report_train['accuracy']]\n",
    "    accuracy_test = [report_test['accuracy'], report_test['accuracy'], report_test['accuracy']]\n",
    "    \n",
    "    report = pd.DataFrame({'Sentiment': labels, 'Precision train': precision_train, 'Precision test': precision_test,\n",
    "                          'Recall train': recall_train, 'Recall test': recall_test, 'F1 train': F1score_train,\n",
    "                          'F1 test': F1score_test, 'Accuracy train': accuracy_train, 'Accuracy test': accuracy_test})\n",
    "    \n",
    "    return report, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f051c1",
   "metadata": {},
   "source": [
    "First, the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ff2ffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for C = 0.01: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4880 2369  532]\n",
      " [1691 8051 1375]\n",
      " [ 587 2385 5610]]\n",
      "Confusion matrix test set: \n",
      " [[ 637  311   53]\n",
      " [ 254 1005  171]\n",
      " [  68  313  722]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.681755        0.664234      0.627169     0.636364   \n",
      "1          0         0.628739        0.616943      0.724206     0.702797   \n",
      "2          1         0.746308        0.763214      0.653694     0.654578   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.653324  0.650000        0.674709        0.66893  \n",
      "1  0.673104  0.657077        0.674709        0.66893  \n",
      "2  0.696938  0.704734        0.674709        0.66893  \n",
      "\n",
      "Results for C = 0.1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[5324 1974  483]\n",
      " [1623 8191 1303]\n",
      " [ 477 1852 6253]]\n",
      "Confusion matrix test set: \n",
      " [[ 687  263   51]\n",
      " [ 255 1005  170]\n",
      " [  55  261  787]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.717134        0.689067      0.684231     0.686314   \n",
      "1          0         0.681618        0.657292      0.736799     0.702797   \n",
      "2          1         0.777833        0.780754      0.728618     0.713509   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.700296  0.687688         0.71936       0.701471  \n",
      "1  0.708135  0.679284         0.71936       0.701471  \n",
      "2  0.752422  0.745618         0.71936       0.701471  \n",
      "\n",
      "Results for C = 1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[5804 1551  426]\n",
      " [1686 8103 1328]\n",
      " [ 418 1303 6861]]\n",
      "Confusion matrix test set: \n",
      " [[717 233  51]\n",
      " [283 961 186]\n",
      " [ 63 218 822]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.733940        0.674506      0.745920     0.716284   \n",
      "1          0         0.739527        0.680595      0.728884     0.672028   \n",
      "2          1         0.796402        0.776204      0.799464     0.745240   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.739881  0.694767         0.75575       0.707414  \n",
      "1  0.734167  0.676284         0.75575       0.707414  \n",
      "2  0.797930  0.760407         0.75575       0.707414  \n",
      "\n",
      "Results for C = 10: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[5930 1449  402]\n",
      " [1753 8021 1343]\n",
      " [ 404 1171 7007]]\n",
      "Confusion matrix test set: \n",
      " [[716 228  57]\n",
      " [297 918 215]\n",
      " [ 64 213 826]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.733276        0.664810      0.762113     0.715285   \n",
      "1          0         0.753783        0.675497      0.721508     0.641958   \n",
      "2          1         0.800617        0.752277      0.816476     0.748867   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.747416  0.689124        0.762664       0.696095  \n",
      "1  0.737292  0.658300        0.762664       0.696095  \n",
      "2  0.808469  0.750568        0.762664       0.696095  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = [0.01, 0.1, 1, 10]\n",
    "\n",
    "for c in C:\n",
    "    \n",
    "    lr = LogisticRegression(C = c, class_weight = 'balanced')\n",
    "    \n",
    "    print(F'Results for C = {c}: \\n')\n",
    "    \n",
    "    print(prediction(lr, X_train, y_train, X_test, y_test)[0])\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714dbd0",
   "metadata": {},
   "source": [
    "Second, the Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcd73bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for alpha = 0.01: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4533 2879  369]\n",
      " [ 933 9127 1057]\n",
      " [ 286 2626 5670]]\n",
      "Confusion matrix test set: \n",
      " [[ 559  402   40]\n",
      " [ 170 1088  172]\n",
      " [  41  394  668]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.788074        0.725974      0.582573     0.558442   \n",
      "1          0         0.623770        0.577495      0.820995     0.760839   \n",
      "2          1         0.799042        0.759091      0.660685     0.605621   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.669918  0.631282        0.703421       0.655065  \n",
      "1  0.708921  0.656608        0.703421       0.655065  \n",
      "2  0.723307  0.673727        0.703421       0.655065  \n",
      "\n",
      "Results for alpha = 0.1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4529 2883  369]\n",
      " [ 929 9132 1056]\n",
      " [ 285 2627 5670]]\n",
      "Confusion matrix test set: \n",
      " [[ 556  405   40]\n",
      " [ 168 1090  172]\n",
      " [  40  394  669]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.788612        0.727749      0.582059     0.555445   \n",
      "1          0         0.623685        0.577025      0.821445     0.762238   \n",
      "2          1         0.799154        0.759364      0.660685     0.606528   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.669772  0.630028        0.703457       0.655065  \n",
      "1  0.709034  0.656824        0.703457       0.655065  \n",
      "2  0.723353  0.674395        0.703457       0.655065  \n",
      "\n",
      "Results for alpha = 1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4392 3022  367]\n",
      " [ 842 9250 1025]\n",
      " [ 250 2677 5655]]\n",
      "Confusion matrix test set: \n",
      " [[ 541  422   38]\n",
      " [ 142 1121  167]\n",
      " [  31  395  677]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.800875        0.757703      0.564452     0.540460   \n",
      "1          0         0.618770        0.578431      0.832059     0.783916   \n",
      "2          1         0.802469        0.767574      0.658937     0.613781   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.662194  0.630904         0.70222       0.661856  \n",
      "1  0.709737  0.665677         0.70222       0.661856  \n",
      "2  0.723655  0.682116         0.70222       0.661856  \n",
      "\n",
      "Results for alpha = 8: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[3282 4172  327]\n",
      " [ 455 9752  910]\n",
      " [ 130 3216 5236]]\n",
      "Confusion matrix test set: \n",
      " [[ 409  563   29]\n",
      " [  78 1225  127]\n",
      " [  13  444  646]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.848720        0.818000      0.421797     0.408591   \n",
      "1          0         0.568961        0.548835      0.877215     0.856643   \n",
      "2          1         0.808899        0.805486      0.610114     0.585675   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.563530  0.544970        0.664847       0.645161  \n",
      "1  0.690236  0.669033        0.664847       0.645161  \n",
      "2  0.695583  0.678215        0.664847       0.645161  \n",
      "\n",
      "Results for alpha = 10: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[3023 4445  313]\n",
      " [ 401 9843  873]\n",
      " [ 110 3352 5120]]\n",
      "Confusion matrix test set: \n",
      " [[ 389  583   29]\n",
      " [  63 1241  126]\n",
      " [  11  453  639]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.855405        0.840173      0.388510     0.388611   \n",
      "1          0         0.557993        0.545015      0.885401     0.867832   \n",
      "2          1         0.811925        0.804786      0.596598     0.579329   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.534335  0.531421        0.654512       0.642049  \n",
      "1  0.684564  0.669544        0.654512       0.642049  \n",
      "2  0.687802  0.673695        0.654512       0.642049  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.01, 0.1, 1, 8, 10]\n",
    "\n",
    "for a in alpha:\n",
    "    \n",
    "    nb = MultinomialNB(alpha = a)\n",
    "    \n",
    "    print(F'Results for alpha = {a}: \\n')\n",
    "\n",
    "    print(prediction(nb, X_train, y_train, X_test, y_test)[0])\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909dc74",
   "metadata": {},
   "source": [
    "Third, the Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "11784285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for max_depth = 5: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[3781 3500  500]\n",
      " [ 866 8986 1265]\n",
      " [ 253 2804 5525]]\n",
      "Confusion matrix test set: \n",
      " [[ 484  460   57]\n",
      " [ 151 1110  169]\n",
      " [  17  375  711]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.771633        0.742331      0.485927     0.483516   \n",
      "1          0         0.587704        0.570694      0.808312     0.776224   \n",
      "2          1         0.757888        0.758805      0.643789     0.644606   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.596325  0.585602        0.665648       0.652235  \n",
      "1  0.680577  0.657778        0.665648       0.652235  \n",
      "2  0.696195  0.697059        0.665648       0.652235  \n",
      "\n",
      "Results for max_depth = 10: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[3865 3444  472]\n",
      " [ 901 9020 1196]\n",
      " [ 271 2495 5816]]\n",
      "Confusion matrix test set: \n",
      " [[ 493  451   57]\n",
      " [ 165 1100  165]\n",
      " [  28  338  737]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.767322        0.718659      0.496723     0.492507   \n",
      "1          0         0.602981        0.582319      0.811370     0.769231   \n",
      "2          1         0.777125        0.768509      0.677698     0.668178   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.603058  0.584469        0.680531        0.65931  \n",
      "1  0.691824  0.662850        0.680531        0.65931  \n",
      "2  0.724013  0.714840        0.680531        0.65931  \n",
      "\n",
      "Results for max_depth = 15: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4032 3302  447]\n",
      " [ 762 9235 1120]\n",
      " [ 231 2366 5985]]\n",
      "Confusion matrix test set: \n",
      " [[ 490  445   66]\n",
      " [ 160 1101  169]\n",
      " [  22  322  759]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.802388        0.729167      0.518185     0.489510   \n",
      "1          0         0.619674        0.589400      0.830710     0.769930   \n",
      "2          1         0.792505        0.763581      0.697390     0.688123   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.629705  0.585774        0.700582       0.664969  \n",
      "1  0.709839  0.667677        0.700582       0.664969  \n",
      "2  0.741911  0.723891        0.700582       0.664969  \n",
      "\n",
      "Results for max_depth = 20: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4103 3280  398]\n",
      " [ 683 9455  979]\n",
      " [ 218 2349 6015]]\n",
      "Confusion matrix test set: \n",
      " [[ 508  440   53]\n",
      " [ 150 1116  164]\n",
      " [  21  320  762]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.819944        0.748159      0.527310     0.507493   \n",
      "1          0         0.626823        0.594883      0.850499     0.780420   \n",
      "2          1         0.813718        0.778345      0.700886     0.690843   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.641846  0.604762        0.712263       0.675156  \n",
      "1  0.721728  0.675136        0.712263       0.675156  \n",
      "2  0.753099  0.731988        0.712263       0.675156  \n",
      "\n",
      "Results for max_depth = 30: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4411 3016  354]\n",
      " [ 507 9723  887]\n",
      " [ 164 2143 6275]]\n",
      "Confusion matrix test set: \n",
      " [[ 527  413   61]\n",
      " [ 164 1106  160]\n",
      " [  19  308  776]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.867965        0.742254      0.566894     0.526474   \n",
      "1          0         0.653340        0.605364      0.874606     0.773427   \n",
      "2          1         0.834886        0.778335      0.731182     0.703536   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.685843  0.616014        0.742686       0.681664  \n",
      "1  0.747952  0.679153        0.742686       0.681664  \n",
      "2  0.779600  0.739048        0.742686       0.681664  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "depth = [5, 10, 15, 20, 30]\n",
    "\n",
    "for d in depth:\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state = 0, n_estimators = 50, max_depth = d, class_weight = 'balanced')\n",
    "    \n",
    "    print(F'Results for max_depth = {d}: \\n')\n",
    "\n",
    "    print(prediction(rf, X_train, y_train, X_test, y_test)[0])\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7364808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix train set: \n",
      " [[5595 1565  621]\n",
      " [1476 8038 1603]\n",
      " [ 458 1250 6874]]\n",
      "Confusion matrix test set: \n",
      " [[666 262  73]\n",
      " [262 960 208]\n",
      " [ 50 208 845]]\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Precision train</th>\n",
       "      <th>Precision test</th>\n",
       "      <th>Recall train</th>\n",
       "      <th>Recall test</th>\n",
       "      <th>F1 train</th>\n",
       "      <th>F1 test</th>\n",
       "      <th>Accuracy train</th>\n",
       "      <th>Accuracy test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.743127</td>\n",
       "      <td>0.680982</td>\n",
       "      <td>0.719059</td>\n",
       "      <td>0.665335</td>\n",
       "      <td>0.730895</td>\n",
       "      <td>0.673067</td>\n",
       "      <td>0.746252</td>\n",
       "      <td>0.699208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.740625</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.723037</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.731725</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.746252</td>\n",
       "      <td>0.699208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.755551</td>\n",
       "      <td>0.750444</td>\n",
       "      <td>0.800979</td>\n",
       "      <td>0.766092</td>\n",
       "      <td>0.777602</td>\n",
       "      <td>0.758188</td>\n",
       "      <td>0.746252</td>\n",
       "      <td>0.699208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
       "0         -1         0.743127        0.680982      0.719059     0.665335   \n",
       "1          0         0.740625        0.671329      0.723037     0.671329   \n",
       "2          1         0.755551        0.750444      0.800979     0.766092   \n",
       "\n",
       "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
       "0  0.730895  0.673067        0.746252       0.699208  \n",
       "1  0.731725  0.671329        0.746252       0.699208  \n",
       "2  0.777602  0.758188        0.746252       0.699208  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state = 0, n_estimators = 50, max_depth = None, class_weight = 'balanced',\n",
    "                               min_samples_leaf = 5)\n",
    "    \n",
    "\n",
    "prediction(rf, X_train, y_train, X_test, y_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072b456",
   "metadata": {},
   "source": [
    "Looking at the results, and allowing a maximum difference of 5 percent points between the accuracies of the training set and the test set to avoid overfitting, we can see that the best model regarding this metric is the Logistic Regression with a parameter C = 1. This gives us about a 70.7% accuracy and the respective precisions, recalls and F1 Scores for each class range from 0.67 to 0.76. This could be considered as an acceptable model, although we may need a more advanced one, for example LSTM or other kinds of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853be67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
