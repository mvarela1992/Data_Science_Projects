{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644bf37b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Hi! Mario here! In this project I'll perform Sentiment Analysis on a Kaggle dataset (https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset/data), which contains the following:\n",
    "\n",
    "1) Text ID\n",
    "2) Text\n",
    "3) Selected text\n",
    "4) Sentiment\n",
    "5) Time of tweet\n",
    "6) Age of User\n",
    "7) Country\n",
    "8) Population\n",
    "9) Land Area\n",
    "10) Density\n",
    "\n",
    "It comes in 2 separate files, one for training and one for testing. Let's first import some packages and load both files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd971a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b9a18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>31072940</td>\n",
       "      <td>227540.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Greece</td>\n",
       "      <td>10423054</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Grenada</td>\n",
       "      <td>112523</td>\n",
       "      <td>340.0</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>17915568</td>\n",
       "      <td>107160.0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>13132795</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \\\n",
       "0                    I`d have responded, if I were going   neutral   \n",
       "1                                               Sooo SAD  negative   \n",
       "2                                            bullying me  negative   \n",
       "3                                         leave me alone  negative   \n",
       "4                                          Sons of ****,  negative   \n",
       "...                                                  ...       ...   \n",
       "27476                                             d lost  negative   \n",
       "27477                                      , don`t force  negative   \n",
       "27478                          Yay good for both of you.  positive   \n",
       "27479                         But it was worth it  ****.  positive   \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral   \n",
       "\n",
       "      Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0           morning        0-20  Afghanistan          38928346   \n",
       "1              noon       21-30      Albania           2877797   \n",
       "2             night       31-45      Algeria          43851044   \n",
       "3           morning       46-60      Andorra             77265   \n",
       "4              noon       60-70       Angola          32866272   \n",
       "...             ...         ...          ...               ...   \n",
       "27476         night       31-45        Ghana          31072940   \n",
       "27477       morning       46-60       Greece          10423054   \n",
       "27478          noon       60-70      Grenada            112523   \n",
       "27479         night      70-100    Guatemala          17915568   \n",
       "27480       morning        0-20       Guinea          13132795   \n",
       "\n",
       "       Land Area (Km²)  Density (P/Km²)  \n",
       "0             652860.0               60  \n",
       "1              27400.0              105  \n",
       "2            2381740.0               18  \n",
       "3                470.0              164  \n",
       "4            1246700.0               26  \n",
       "...                ...              ...  \n",
       "27476         227540.0              137  \n",
       "27477         128900.0               81  \n",
       "27478            340.0              331  \n",
       "27479         107160.0              167  \n",
       "27480         246000.0               53  \n",
       "\n",
       "[27481 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Sentiment_train.csv', encoding='unicode_escape')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b2e44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346.0</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797.0</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044.0</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272.0</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4810</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4811</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4812</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4814</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4815 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment  \\\n",
       "0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1     96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3     01082688c6                                        happy bday!  positive   \n",
       "4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "...          ...                                                ...       ...   \n",
       "4810         NaN                                                NaN       NaN   \n",
       "4811         NaN                                                NaN       NaN   \n",
       "4812         NaN                                                NaN       NaN   \n",
       "4813         NaN                                                NaN       NaN   \n",
       "4814         NaN                                                NaN       NaN   \n",
       "\n",
       "     Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0          morning        0-20  Afghanistan        38928346.0   \n",
       "1             noon       21-30      Albania         2877797.0   \n",
       "2            night       31-45      Algeria        43851044.0   \n",
       "3          morning       46-60      Andorra           77265.0   \n",
       "4             noon       60-70       Angola        32866272.0   \n",
       "...            ...         ...          ...               ...   \n",
       "4810           NaN         NaN          NaN               NaN   \n",
       "4811           NaN         NaN          NaN               NaN   \n",
       "4812           NaN         NaN          NaN               NaN   \n",
       "4813           NaN         NaN          NaN               NaN   \n",
       "4814           NaN         NaN          NaN               NaN   \n",
       "\n",
       "      Land Area (Km²)  Density (P/Km²)  \n",
       "0            652860.0             60.0  \n",
       "1             27400.0            105.0  \n",
       "2           2381740.0             18.0  \n",
       "3               470.0            164.0  \n",
       "4           1246700.0             26.0  \n",
       "...               ...              ...  \n",
       "4810              NaN              NaN  \n",
       "4811              NaN              NaN  \n",
       "4812              NaN              NaN  \n",
       "4813              NaN              NaN  \n",
       "4814              NaN              NaN  \n",
       "\n",
       "[4815 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('Sentiment_test.csv', encoding='unicode_escape')\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fbf11cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            27481 non-null  object \n",
      " 1   text              27480 non-null  object \n",
      " 2   selected_text     27480 non-null  object \n",
      " 3   sentiment         27481 non-null  object \n",
      " 4   Time of Tweet     27481 non-null  object \n",
      " 5   Age of User       27481 non-null  object \n",
      " 6   Country           27481 non-null  object \n",
      " 7   Population -2020  27481 non-null  int64  \n",
      " 8   Land Area (Km²)   27481 non-null  float64\n",
      " 9   Density (P/Km²)   27481 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd42e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4815 entries, 0 to 4814\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   textID            3534 non-null   object \n",
      " 1   text              3534 non-null   object \n",
      " 2   sentiment         3534 non-null   object \n",
      " 3   Time of Tweet     3534 non-null   object \n",
      " 4   Age of User       3534 non-null   object \n",
      " 5   Country           3534 non-null   object \n",
      " 6   Population -2020  3534 non-null   float64\n",
      " 7   Land Area (Km²)   3534 non-null   float64\n",
      " 8   Density (P/Km²)   3534 non-null   float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 338.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53cb96",
   "metadata": {},
   "source": [
    "Let's see the frequency of each sentiment in the training set. Also, we can see that the test set has some null values in that column, so let's see the null values in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caccae17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f96f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the training set: \n",
      " textID              0\n",
      "text                1\n",
      "selected_text       1\n",
      "sentiment           0\n",
      "Time of Tweet       0\n",
      "Age of User         0\n",
      "Country             0\n",
      "Population -2020    0\n",
      "Land Area (Km²)     0\n",
      "Density (P/Km²)     0\n",
      "dtype: int64\n",
      "\n",
      "Null values in the test set: \n",
      " textID              1281\n",
      "text                1281\n",
      "sentiment           1281\n",
      "Time of Tweet       1281\n",
      "Age of User         1281\n",
      "Country             1281\n",
      "Population -2020    1281\n",
      "Land Area (Km²)     1281\n",
      "Density (P/Km²)     1281\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Null values in the training set: \\n', train.isnull().sum())\n",
    "print('')\n",
    "print('Null values in the test set: \\n', test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd09c7",
   "metadata": {},
   "source": [
    "The classes in the training set are quite balanced. Also, there are a lot of null values in the test set, and only 1 for the train set. Clearly they correspond to important columns in order to perform the sentiment analysis, so we'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcdaa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the training set: \n",
      " textID              0\n",
      "text                0\n",
      "selected_text       0\n",
      "sentiment           0\n",
      "Time of Tweet       0\n",
      "Age of User         0\n",
      "Country             0\n",
      "Population -2020    0\n",
      "Land Area (Km²)     0\n",
      "Density (P/Km²)     0\n",
      "dtype: int64\n",
      "\n",
      "Null values in the test set: \n",
      " textID              0\n",
      "text                0\n",
      "sentiment           0\n",
      "Time of Tweet       0\n",
      "Age of User         0\n",
      "Country             0\n",
      "Population -2020    0\n",
      "Land Area (Km²)     0\n",
      "Density (P/Km²)     0\n",
      "dtype: int64\n",
      "\n",
      "Rows number in the training set:  27480\n",
      "Rows number in the test set:  3534\n"
     ]
    }
   ],
   "source": [
    "train = train.dropna()\n",
    "\n",
    "test = test.dropna()\n",
    "\n",
    "print('Null values in the training set: \\n', train.isnull().sum())\n",
    "print('')\n",
    "print('Null values in the test set: \\n', test.isnull().sum())\n",
    "print('')\n",
    "print('Rows number in the training set: ', len(train))\n",
    "print('Rows number in the test set: ', len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b722d",
   "metadata": {},
   "source": [
    "Now we don't have any null values, we have a training dataset consisting of 27,480 rows and a test dataset of 3,534. Now we can begin the text analysis. First, we need the sentiment labels to be numeric, so I'll create a function that assign the following values: negative = -1, neutral = 0, positive = 1. Then, I'll apply it to both Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259e0c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1   -1\n",
      "2   -1\n",
      "3   -1\n",
      "4   -1\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "0    0\n",
      "1    1\n",
      "2   -1\n",
      "3    1\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def sentiment_encoding(sentiment):\n",
    "    \n",
    "    if sentiment == 'negative':\n",
    "        return -1\n",
    "    \n",
    "    elif sentiment == 'neutral':\n",
    "        return 0\n",
    "    \n",
    "    elif sentiment == 'positive':\n",
    "        return 1\n",
    "    \n",
    "#Apply function to the train and test sets.\n",
    "train['sentiment'] = train['sentiment'].apply(lambda x: sentiment_encoding(x))\n",
    "test['sentiment'] = test['sentiment'].apply(lambda x: sentiment_encoding(x))\n",
    "\n",
    "print(train['sentiment'].head())\n",
    "print('')\n",
    "print(test['sentiment'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c78a7",
   "metadata": {},
   "source": [
    "We now have the sentiment labels numerically encoded. Our next step is to preprocess the text. In the training set I'll just use the raw text, because it's closer to reality. The first thing I'll do is to convert everything to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b49411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: ' '.join(x.lower() for x in str(x).split()))\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join(x.lower() for x in str(x).split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f1d7a",
   "metadata": {},
   "source": [
    "Let's remove the HTML tags and websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36700168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "test['text'] = test['text'].apply(lambda x: BeautifulSoup(x).get_text())\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0bc29",
   "metadata": {},
   "source": [
    "Let's expand the abbreviations of some terms. For example, from \"I won't\" to \"I will not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da69eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(s):\n",
    "\n",
    "    s = re.sub(r\"won't\", \"will not\",s)\n",
    "    s = re.sub(r\"would't\", \"would not\",s)\n",
    "    s = re.sub(r\"could't\", \"could not\",s)\n",
    "    s = re.sub(r\"\\'d\", \" would\",s)\n",
    "    s = re.sub(r\"can\\'t\", \"can not\",s)\n",
    "    s = re.sub(r\"n\\'t\", \" not\", s)\n",
    "    s= re.sub(r\"\\'re\", \" are\", s)\n",
    "    s = re.sub(r\"\\'s\", \" is\", s)\n",
    "    s = re.sub(r\"\\'ll\", \" will\", s)\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    s = re.sub(r\"\\'ve\", \" have\", s)\n",
    "    s = re.sub(r\"\\'m\", \" am\", s)\n",
    " \n",
    "    return s\n",
    "\n",
    "train['text']  = train['text'].apply(lambda x: expand(x))  \n",
    "test['text']  = test['text'].apply(lambda x: expand(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0deb05",
   "metadata": {},
   "source": [
    "Let's remove non alpha-numeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e87f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)])) \n",
    "test['text'] = test['text'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243a667",
   "metadata": {},
   "source": [
    "Now let's remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac10b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "train['text'] = train['text'].apply(lambda x: ' '.join([x for x in x.split() if x not in stop]))\n",
    "test['text'] = test['text'].apply(lambda x: ' '.join([x for x in x.split() if x not in stop]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ba001",
   "metadata": {},
   "source": [
    "Time to lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb9f302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>responded going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>-1</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>bos bullying</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>-1</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>interview leave alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>-1</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>son put release already bought</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>-1</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish could come see u denver husband lost job ...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>-1</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>31072940</td>\n",
       "      <td>227540.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>wondered rake client made clear net force devs...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>-1</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Greece</td>\n",
       "      <td>10423054</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>yay good enjoy break probably need hectic week...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>1</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Grenada</td>\n",
       "      <td>112523</td>\n",
       "      <td>340.0</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>worth</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>17915568</td>\n",
       "      <td>107160.0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>flirting going atg smile yay hug</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>13132795</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                                    responded going   \n",
       "1      549e992a42                            sooo sad miss san diego   \n",
       "2      088c60f138                                       bos bullying   \n",
       "3      9642c003ef                              interview leave alone   \n",
       "4      358bd9e861                     son put release already bought   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0  wish could come see u denver husband lost job ...   \n",
       "27477  4f4c4fc327  wondered rake client made clear net force devs...   \n",
       "27478  f67aae2310  yay good enjoy break probably need hectic week...   \n",
       "27479  ed167662a5                                              worth   \n",
       "27480  6f7127d9d7                   flirting going atg smile yay hug   \n",
       "\n",
       "                                           selected_text  sentiment  \\\n",
       "0                    I`d have responded, if I were going          0   \n",
       "1                                               Sooo SAD         -1   \n",
       "2                                            bullying me         -1   \n",
       "3                                         leave me alone         -1   \n",
       "4                                          Sons of ****,         -1   \n",
       "...                                                  ...        ...   \n",
       "27476                                             d lost         -1   \n",
       "27477                                      , don`t force         -1   \n",
       "27478                          Yay good for both of you.          1   \n",
       "27479                         But it was worth it  ****.          1   \n",
       "27480  All this flirting going on - The ATG smiles. Y...          0   \n",
       "\n",
       "      Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0           morning        0-20  Afghanistan          38928346   \n",
       "1              noon       21-30      Albania           2877797   \n",
       "2             night       31-45      Algeria          43851044   \n",
       "3           morning       46-60      Andorra             77265   \n",
       "4              noon       60-70       Angola          32866272   \n",
       "...             ...         ...          ...               ...   \n",
       "27476         night       31-45        Ghana          31072940   \n",
       "27477       morning       46-60       Greece          10423054   \n",
       "27478          noon       60-70      Grenada            112523   \n",
       "27479         night      70-100    Guatemala          17915568   \n",
       "27480       morning        0-20       Guinea          13132795   \n",
       "\n",
       "       Land Area (Km²)  Density (P/Km²)  \n",
       "0             652860.0               60  \n",
       "1              27400.0              105  \n",
       "2            2381740.0               18  \n",
       "3                470.0              164  \n",
       "4            1246700.0               26  \n",
       "...                ...              ...  \n",
       "27476         227540.0              137  \n",
       "27477         128900.0               81  \n",
       "27478            340.0              331  \n",
       "27479         107160.0              167  \n",
       "27480         246000.0               53  \n",
       "\n",
       "[27480 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train['text'] = train['text'].apply(lambda x: \n",
    "                                            ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "test['text'] = test['text'].apply(lambda x: \n",
    "                                            ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73053a21",
   "metadata": {},
   "source": [
    "We can't classify the text unless it has a numeric format. In order to achieve that, I'll use the method called TF-IDF, which is a numeric measure that expresses how important is a word for a document in a collection. Its use is more recommended than the normal Count Vectorizer. First we convert the training and test texts to a list format and then apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951083a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['text'].astype(str).tolist()\n",
    "test_text = test['text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecc71777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Create the X_train and X_test sets.\n",
    "X_train = vectorizer.fit_transform(train_text).toarray()\n",
    "\n",
    "X_test = vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b04de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480, 22570)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf285c38",
   "metadata": {},
   "source": [
    "Let's create the target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00d97155",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d153d50",
   "metadata": {},
   "source": [
    "Now I'll try 3 models, a Logistic Regression, a Multinomial Naive Bayes (widely used in text mining) and a Random Forest, for different values of C, alpha and maximum depth, respectively. Both Logistic Regression and Random Forest will have a balanced class weight. From there I'll decide which one performs better. First, a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c27732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print('Confusion matrix train set: \\n', confusion_matrix(y_train, y_pred_train))\n",
    "    print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred))\n",
    "    print(' ')\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict = True)\n",
    "    report_test = classification_report(y_test, y_pred, output_dict = True)\n",
    "    \n",
    "    labels = [-1, 0, 1]\n",
    "    precision_train = [report_train['-1']['precision'], report_train['0']['precision'], report_train['1']['precision']]\n",
    "    precision_test = [report_test['-1']['precision'], report_test['0']['precision'], report_test['1']['precision']]\n",
    "    \n",
    "    recall_train = [report_train['-1']['recall'], report_train['0']['recall'], report_train['1']['recall']]\n",
    "    recall_test = [report_test['-1']['recall'], report_test['0']['recall'], report_test['1']['recall']]\n",
    "    \n",
    "    F1score_train = [report_train['-1']['f1-score'], report_train['0']['f1-score'], report_train['1']['f1-score']]\n",
    "    F1score_test = [report_test['-1']['f1-score'], report_test['0']['f1-score'], report_test['1']['f1-score']]\n",
    "    \n",
    "    accuracy_train = [report_train['accuracy'], report_train['accuracy'], report_train['accuracy']]\n",
    "    accuracy_test = [report_test['accuracy'], report_test['accuracy'], report_test['accuracy']]\n",
    "    \n",
    "    report = pd.DataFrame({'Sentiment': labels, 'Precision train': precision_train, 'Precision test': precision_test,\n",
    "                          'Recall train': recall_train, 'Recall test': recall_test, 'F1 train': F1score_train,\n",
    "                          'F1 test': F1score_test, 'Accuracy train': accuracy_train, 'Accuracy test': accuracy_test})\n",
    "    \n",
    "    return report, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f051c1",
   "metadata": {},
   "source": [
    "First, the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff2ffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for C = 0.01: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4887 2375  519]\n",
      " [1591 8217 1309]\n",
      " [ 582 2518 5482]]\n",
      "Confusion matrix test set: \n",
      " [[ 631  316   54]\n",
      " [ 254 1009  167]\n",
      " [  71  319  713]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.692210        0.660042      0.628068     0.630370   \n",
      "1          0         0.626773        0.613747      0.739138     0.705594   \n",
      "2          1         0.749932        0.763383      0.638779     0.646419   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.658581  0.644865        0.676346       0.665818  \n",
      "1  0.678334  0.656474        0.676346       0.665818  \n",
      "2  0.689907  0.700049        0.676346       0.665818  \n",
      "\n",
      "Results for C = 0.1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[5386 1962  433]\n",
      " [1424 8471 1222]\n",
      " [ 437 1928 6217]]\n",
      "Confusion matrix test set: \n",
      " [[ 666  284   51]\n",
      " [ 253 1012  165]\n",
      " [  62  260  781]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.743204        0.678899      0.692199     0.665335   \n",
      "1          0         0.685301        0.650386      0.761986     0.707692   \n",
      "2          1         0.789761        0.783350      0.724423     0.708069   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.716795  0.672048        0.730495       0.695812  \n",
      "1  0.721612  0.677830        0.730495       0.695812  \n",
      "2  0.755683  0.743810        0.730495       0.695812  \n",
      "\n",
      "Results for C = 1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[6550  912  319]\n",
      " [1148 8948 1021]\n",
      " [ 293 1012 7277]]\n",
      "Confusion matrix test set: \n",
      " [[713 246  42]\n",
      " [287 956 187]\n",
      " [ 66 221 816]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.819672        0.668856      0.841794     0.712288   \n",
      "1          0         0.823032        0.671820      0.804893     0.668531   \n",
      "2          1         0.844493        0.780861      0.847938     0.739801   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.830586  0.689889        0.828785       0.703169  \n",
      "1  0.813861  0.670172        0.828785       0.703169  \n",
      "2  0.846212  0.759777        0.828785       0.703169  \n",
      "\n",
      "Results for C = 10: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[7216  384  181]\n",
      " [ 787 9669  661]\n",
      " [ 166  362 8054]]\n",
      "Confusion matrix test set: \n",
      " [[709 239  53]\n",
      " [311 897 222]\n",
      " [ 69 229 805]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.883339        0.651056      0.927387     0.708292   \n",
      "1          0         0.928373        0.657143      0.869749     0.627273   \n",
      "2          1         0.905351        0.745370      0.938476     0.729828   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.904828  0.678469        0.907533        0.68223  \n",
      "1  0.898105  0.641860        0.907533        0.68223  \n",
      "2  0.921616  0.737517        0.907533        0.68223  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = [0.01, 0.1, 1, 10]\n",
    "\n",
    "for c in C:\n",
    "    \n",
    "    lr = LogisticRegression(C = c, class_weight = 'balanced')\n",
    "    \n",
    "    print(F'Results for C = {c}: \\n')\n",
    "    \n",
    "    print(prediction(lr, X_train, y_train, X_test, y_test)[0])\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714dbd0",
   "metadata": {},
   "source": [
    "Second, the Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd73bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for alpha = 0.01: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[ 6554  1026   201]\n",
      " [  504 10021   592]\n",
      " [  185   911  7486]]\n",
      "Confusion matrix test set: \n",
      " [[555 373  73]\n",
      " [267 907 256]\n",
      " [ 92 372 639]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.904874        0.607221      0.842308     0.554446   \n",
      "1          0         0.838016        0.549031      0.901412     0.634266   \n",
      "2          1         0.904215        0.660124      0.872291     0.579329   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.872471  0.579634        0.875582        0.59451  \n",
      "1  0.868559  0.588579        0.875582        0.59451  \n",
      "2  0.887966  0.617093        0.875582        0.59451  \n",
      "\n",
      "Results for alpha = 0.1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[ 6385  1181   215]\n",
      " [  486 10045   586]\n",
      " [  183  1038  7361]]\n",
      "Confusion matrix test set: \n",
      " [[553 388  60]\n",
      " [248 945 237]\n",
      " [ 73 373 657]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.905160        0.632723      0.820589     0.552448   \n",
      "1          0         0.819064        0.553927      0.903571     0.660839   \n",
      "2          1         0.901862        0.688679      0.857725     0.595648   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.860802  0.589867        0.865757       0.609791  \n",
      "1  0.859245  0.602679        0.865757       0.609791  \n",
      "2  0.879240  0.638794        0.865757       0.609791  \n",
      "\n",
      "Results for alpha = 1: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[ 4893  2635   253]\n",
      " [  302 10258   557]\n",
      " [  117  2030  6435]]\n",
      "Confusion matrix test set: \n",
      " [[ 437  535   29]\n",
      " [ 119 1151  160]\n",
      " [  29  441  633]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.921122        0.747009      0.628839     0.436563   \n",
      "1          0         0.687395        0.541138      0.922731     0.804895   \n",
      "2          1         0.888199        0.770073      0.749825     0.573889   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.747422  0.551072        0.785517       0.628466  \n",
      "1  0.787865  0.647175        0.785517       0.628466  \n",
      "2  0.813167  0.657662        0.785517       0.628466  \n",
      "\n",
      "Results for alpha = 8: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[ 1968  5612   201]\n",
      " [  111 10519   487]\n",
      " [   36  3966  4580]]\n",
      "Confusion matrix test set: \n",
      " [[ 244  741   16]\n",
      " [  18 1336   76]\n",
      " [   5  558  540]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.930496        0.913858      0.252924     0.243756   \n",
      "1          0         0.523411        0.507021      0.946209     0.934266   \n",
      "2          1         0.869400        0.854430      0.533675     0.489574   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.397736  0.384858         0.62107       0.599887  \n",
      "1  0.673992  0.657319         0.62107       0.599887  \n",
      "2  0.661372  0.622478         0.62107       0.599887  \n",
      "\n",
      "Results for alpha = 10: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[ 1731  5863   187]\n",
      " [   95 10542   480]\n",
      " [   30  4155  4397]]\n",
      "Confusion matrix test set: \n",
      " [[ 215  769   17]\n",
      " [  15 1340   75]\n",
      " [   3  569  531]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.932651        0.922747      0.222465     0.214785   \n",
      "1          0         0.512743        0.500373      0.948277     0.937063   \n",
      "2          1         0.868286        0.852327      0.512351     0.481414   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.359240  0.348460        0.606623       0.590266  \n",
      "1  0.665593  0.652386        0.606623       0.590266  \n",
      "2  0.644438  0.615295        0.606623       0.590266  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.01, 0.1, 1, 8, 10]\n",
    "\n",
    "for a in alpha:\n",
    "    \n",
    "    nb = MultinomialNB(alpha = a)\n",
    "    \n",
    "    print(F'Results for alpha = {a}: \\n')\n",
    "\n",
    "    print(prediction(nb, X_train, y_train, X_test, y_test)[0])\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909dc74",
   "metadata": {},
   "source": [
    "Third, the Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11784285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for max_depth = 5: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4021 3188  572]\n",
      " [1580 8101 1436]\n",
      " [ 697 3597 4288]]\n",
      "Confusion matrix test set: \n",
      " [[536 399  66]\n",
      " [264 986 180]\n",
      " [ 97 507 499]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.638457        0.597547      0.516772     0.535465   \n",
      "1          0         0.544203        0.521142      0.728704     0.689510   \n",
      "2          1         0.681067        0.669799      0.499650     0.452403   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.571205  0.564805        0.597162       0.571873  \n",
      "1  0.623082  0.593618        0.597162       0.571873  \n",
      "2  0.576422  0.540043        0.597162       0.571873  \n",
      "\n",
      "Results for max_depth = 10: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4753 2541  487]\n",
      " [1282 8426 1409]\n",
      " [ 415 2529 5638]]\n",
      "Confusion matrix test set: \n",
      " [[607 341  53]\n",
      " [248 993 189]\n",
      " [ 50 360 693]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.736899        0.670718      0.610847     0.606394   \n",
      "1          0         0.624333        0.586187      0.757938     0.694406   \n",
      "2          1         0.748341        0.741176      0.656956     0.628286   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.667978  0.636936        0.684753        0.64884  \n",
      "1  0.684679  0.635723        0.684753        0.64884  \n",
      "2  0.699677  0.680079        0.684753        0.64884  \n",
      "\n",
      "Results for max_depth = 15: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[4917 2378  486]\n",
      " [1150 8641 1326]\n",
      " [ 352 1986 6244]]\n",
      "Confusion matrix test set: \n",
      " [[617 322  62]\n",
      " [238 996 196]\n",
      " [ 47 275 781]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.766007        0.684035      0.631924     0.616384   \n",
      "1          0         0.664437        0.625235      0.777278     0.696503   \n",
      "2          1         0.775074        0.751684      0.727569     0.708069   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.692535  0.648450        0.720597       0.677419  \n",
      "1  0.716441  0.658948        0.720597       0.677419  \n",
      "2  0.750571  0.729225        0.720597       0.677419  \n",
      "\n",
      "Results for max_depth = 20: \n",
      "\n",
      "Confusion matrix train set: \n",
      " [[5091 2191  499]\n",
      " [1035 8754 1328]\n",
      " [ 336 1848 6398]]\n",
      "Confusion matrix test set: \n",
      " [[ 610  328   63]\n",
      " [ 214 1003  213]\n",
      " [  49  257  797]]\n",
      " \n",
      "   Sentiment  Precision train  Precision test  Recall train  Recall test  \\\n",
      "0         -1         0.787837        0.698740      0.654286     0.609391   \n",
      "1          0         0.684280        0.631612      0.787443     0.701399   \n",
      "2          1         0.777872        0.742777      0.745514     0.722575   \n",
      "\n",
      "   F1 train   F1 test  Accuracy train  Accuracy test  \n",
      "0  0.714877  0.651014        0.736645       0.681947  \n",
      "1  0.732246  0.664679        0.736645       0.681947  \n",
      "2  0.761349  0.732537        0.736645       0.681947  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "depth = [5, 10, 15, 20]\n",
    "\n",
    "for d in depth:\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state = 0, n_estimators = 50, max_depth = d, class_weight = 'balanced')\n",
    "    \n",
    "    print(F'Results for max_depth = {d}: \\n')\n",
    "\n",
    "    print(prediction(rf, X_train, y_train, X_test, y_test)[0])\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a072b456",
   "metadata": {},
   "source": [
    "Looking at the results, and allowing a maximum difference of 5 percent points between the accuracies of the training set and the test set to avoid overfitting, we can see that the best model regarding this metric is the Logistic Regression with a parameter C = 0.1. This gives us a 70% accuracy and the respective precisions, recalls and F1 Scores for each class range from 0.65 to 0.74. This could be considered as an acceptable model, although we may need a more advanced one, for example LSTM or other kinds of neural networks. I still have yet to learn those, and tensorflow, so I'll update this notebook in the future. For now, this is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f7162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
